{
  "config_general": {
    "lighteval_sha": "?",
    "num_fewshot_seeds": 1,
    "override_batch_size": 1,
    "max_samples": 1000,
    "job_id": "",
    "start_time": 464795.780605839,
    "end_time": 471344.074829044,
    "total_evaluation_time_secondes": "6548.294223205012",
    "model_name": "HuggingFaceFW/ablation-model-fineweb-edu",
    "model_sha": "c0af57759d3534d15f9112bbedc4f9569cdeeb28",
    "model_dtype": "torch.bfloat16",
    "model_size": "3.19 GB",
    "config": null
  },
  "results": {
    "custom|arc:challenge|0": {
      "acc": 0.312,
      "acc_stderr": 0.014658474370509007,
      "acc_norm": 0.329,
      "acc_norm_stderr": 0.014865395385928362
    },
    "custom|arc:easy|0": {
      "acc": 0.666,
      "acc_stderr": 0.014922019523732963,
      "acc_norm": 0.635,
      "acc_norm_stderr": 0.015231776226264896
    },
    "custom|commonsense_qa|0": {
      "acc": 0.346,
      "acc_stderr": 0.015050266127564438,
      "acc_norm": 0.317,
      "acc_norm_stderr": 0.01472167543888024
    },
    "custom|hellaswag|0": {
      "acc": 0.389,
      "acc_stderr": 0.015424555647308493,
      "acc_norm": 0.453,
      "acc_norm_stderr": 0.015749255189977596
    },
    "custom|mmlu:abstract_algebra|0": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909284
    },
    "custom|mmlu:anatomy|0": {
      "acc": 0.3851851851851852,
      "acc_stderr": 0.04203921040156279,
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.039992628766177214
    },
    "custom|mmlu:astronomy|0": {
      "acc": 0.3223684210526316,
      "acc_stderr": 0.03803510248351585,
      "acc_norm": 0.4276315789473684,
      "acc_norm_stderr": 0.040260970832965565
    },
    "custom|mmlu:business_ethics|0": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145633
    },
    "custom|mmlu:clinical_knowledge|0": {
      "acc": 0.2792452830188679,
      "acc_stderr": 0.027611163402399715,
      "acc_norm": 0.3849056603773585,
      "acc_norm_stderr": 0.029946498567699955
    },
    "custom|mmlu:college_biology|0": {
      "acc": 0.3680555555555556,
      "acc_stderr": 0.0403299905396072,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.04076663253918567
    },
    "custom|mmlu:college_chemistry|0": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "custom|mmlu:college_computer_science|0": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768079
    },
    "custom|mmlu:college_mathematics|0": {
      "acc": 0.17,
      "acc_stderr": 0.0377525168068637,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "custom|mmlu:college_medicine|0": {
      "acc": 0.2832369942196532,
      "acc_stderr": 0.034355680560478746,
      "acc_norm": 0.27167630057803466,
      "acc_norm_stderr": 0.0339175032232166
    },
    "custom|mmlu:college_physics|0": {
      "acc": 0.12745098039215685,
      "acc_stderr": 0.033182249219420756,
      "acc_norm": 0.12745098039215685,
      "acc_norm_stderr": 0.033182249219420756
    },
    "custom|mmlu:computer_security|0": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768077,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "custom|mmlu:conceptual_physics|0": {
      "acc": 0.3659574468085106,
      "acc_stderr": 0.0314895582974553,
      "acc_norm": 0.33191489361702126,
      "acc_norm_stderr": 0.03078373675774565
    },
    "custom|mmlu:econometrics|0": {
      "acc": 0.21929824561403508,
      "acc_stderr": 0.03892431106518755,
      "acc_norm": 0.24561403508771928,
      "acc_norm_stderr": 0.040493392977481425
    },
    "custom|mmlu:electrical_engineering|0": {
      "acc": 0.2827586206896552,
      "acc_stderr": 0.03752833958003337,
      "acc_norm": 0.2896551724137931,
      "acc_norm_stderr": 0.037800192304380156
    },
    "custom|mmlu:elementary_mathematics|0": {
      "acc": 0.23544973544973544,
      "acc_stderr": 0.02185150982203171,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.02201908001221789
    },
    "custom|mmlu:formal_logic|0": {
      "acc": 0.2698412698412698,
      "acc_stderr": 0.039701582732351706,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.03970158273235172
    },
    "custom|mmlu:global_facts|0": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "custom|mmlu:high_school_biology|0": {
      "acc": 0.33548387096774196,
      "acc_stderr": 0.02686020644472436,
      "acc_norm": 0.36774193548387096,
      "acc_norm_stderr": 0.02743086657997347
    },
    "custom|mmlu:high_school_chemistry|0": {
      "acc": 0.21674876847290642,
      "acc_stderr": 0.028990331252516235,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.03144712581678241
    },
    "custom|mmlu:high_school_computer_science|0": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "custom|mmlu:high_school_european_history|0": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.035886248000917075,
      "acc_norm": 0.46060606060606063,
      "acc_norm_stderr": 0.03892207016552013
    },
    "custom|mmlu:high_school_geography|0": {
      "acc": 0.3383838383838384,
      "acc_stderr": 0.033711241426263035,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03427308652999935
    },
    "custom|mmlu:high_school_government_and_politics|0": {
      "acc": 0.3471502590673575,
      "acc_stderr": 0.03435696168361355,
      "acc_norm": 0.40414507772020725,
      "acc_norm_stderr": 0.0354150857888402
    },
    "custom|mmlu:high_school_macroeconomics|0": {
      "acc": 0.2692307692307692,
      "acc_stderr": 0.022489389793654817,
      "acc_norm": 0.28205128205128205,
      "acc_norm_stderr": 0.022815813098896597
    },
    "custom|mmlu:high_school_mathematics|0": {
      "acc": 0.13333333333333333,
      "acc_stderr": 0.02072618044813386,
      "acc_norm": 0.16666666666666666,
      "acc_norm_stderr": 0.022722578464550516
    },
    "custom|mmlu:high_school_microeconomics|0": {
      "acc": 0.31932773109243695,
      "acc_stderr": 0.030283995525884396,
      "acc_norm": 0.39915966386554624,
      "acc_norm_stderr": 0.031811100324139245
    },
    "custom|mmlu:high_school_physics|0": {
      "acc": 0.24503311258278146,
      "acc_stderr": 0.03511807571804726,
      "acc_norm": 0.24503311258278146,
      "acc_norm_stderr": 0.035118075718047245
    },
    "custom|mmlu:high_school_psychology|0": {
      "acc": 0.44770642201834865,
      "acc_stderr": 0.02131975496242545,
      "acc_norm": 0.4073394495412844,
      "acc_norm_stderr": 0.021065986244412884
    },
    "custom|mmlu:high_school_statistics|0": {
      "acc": 0.26851851851851855,
      "acc_stderr": 0.0302252261600124,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.030998666304560534
    },
    "custom|mmlu:high_school_us_history|0": {
      "acc": 0.3382352941176471,
      "acc_stderr": 0.033205746129454324,
      "acc_norm": 0.39215686274509803,
      "acc_norm_stderr": 0.03426712349247272
    },
    "custom|mmlu:high_school_world_history|0": {
      "acc": 0.3080168776371308,
      "acc_stderr": 0.030052389335605695,
      "acc_norm": 0.35443037974683544,
      "acc_norm_stderr": 0.031137304297185798
    },
    "custom|mmlu:human_aging|0": {
      "acc": 0.39461883408071746,
      "acc_stderr": 0.03280400504755291,
      "acc_norm": 0.3452914798206278,
      "acc_norm_stderr": 0.03191100192835795
    },
    "custom|mmlu:human_sexuality|0": {
      "acc": 0.40458015267175573,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.3893129770992366,
      "acc_norm_stderr": 0.04276486542814591
    },
    "custom|mmlu:international_law|0": {
      "acc": 0.18181818181818182,
      "acc_stderr": 0.03520893951097652,
      "acc_norm": 0.30578512396694213,
      "acc_norm_stderr": 0.04205953933884124
    },
    "custom|mmlu:jurisprudence|0": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.041331194402438376,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.04414343666854933
    },
    "custom|mmlu:logical_fallacies|0": {
      "acc": 0.32515337423312884,
      "acc_stderr": 0.036803503712864595,
      "acc_norm": 0.32515337423312884,
      "acc_norm_stderr": 0.0368035037128646
    },
    "custom|mmlu:machine_learning|0": {
      "acc": 0.23214285714285715,
      "acc_stderr": 0.04007341809755805,
      "acc_norm": 0.2767857142857143,
      "acc_norm_stderr": 0.04246624336697624
    },
    "custom|mmlu:management|0": {
      "acc": 0.3592233009708738,
      "acc_stderr": 0.047504583990416925,
      "acc_norm": 0.4077669902912621,
      "acc_norm_stderr": 0.04865777570410769
    },
    "custom|mmlu:marketing|0": {
      "acc": 0.452991452991453,
      "acc_stderr": 0.0326109987309862,
      "acc_norm": 0.4188034188034188,
      "acc_norm_stderr": 0.03232128912157792
    },
    "custom|mmlu:medical_genetics|0": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "custom|mmlu:miscellaneous|0": {
      "acc": 0.44189016602809705,
      "acc_stderr": 0.01775880053421441,
      "acc_norm": 0.421455938697318,
      "acc_norm_stderr": 0.017657976412654854
    },
    "custom|mmlu:moral_disputes|0": {
      "acc": 0.2774566473988439,
      "acc_stderr": 0.024105712607754307,
      "acc_norm": 0.22832369942196531,
      "acc_norm_stderr": 0.022598703804321607
    },
    "custom|mmlu:moral_scenarios|0": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "custom|mmlu:nutrition|0": {
      "acc": 0.27124183006535946,
      "acc_stderr": 0.025457756696667874,
      "acc_norm": 0.3464052287581699,
      "acc_norm_stderr": 0.02724561304721535
    },
    "custom|mmlu:philosophy|0": {
      "acc": 0.29260450160771706,
      "acc_stderr": 0.025839898334877983,
      "acc_norm": 0.29260450160771706,
      "acc_norm_stderr": 0.02583989833487798
    },
    "custom|mmlu:prehistory|0": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.02712511551316686,
      "acc_norm": 0.30246913580246915,
      "acc_norm_stderr": 0.025557653981868052
    },
    "custom|mmlu:professional_accounting|0": {
      "acc": 0.2801418439716312,
      "acc_stderr": 0.026789172351140245,
      "acc_norm": 0.25177304964539005,
      "acc_norm_stderr": 0.025892151156709405
    },
    "custom|mmlu:professional_law|0": {
      "acc": 0.248,
      "acc_stderr": 0.013663187134877665,
      "acc_norm": 0.268,
      "acc_norm_stderr": 0.014013292702729475
    },
    "custom|mmlu:professional_medicine|0": {
      "acc": 0.27205882352941174,
      "acc_stderr": 0.027033041151681456,
      "acc_norm": 0.31985294117647056,
      "acc_norm_stderr": 0.02833295951403123
    },
    "custom|mmlu:professional_psychology|0": {
      "acc": 0.3006535947712418,
      "acc_stderr": 0.018550634502952964,
      "acc_norm": 0.29901960784313725,
      "acc_norm_stderr": 0.018521756215423024
    },
    "custom|mmlu:public_relations|0": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.047093069786618966,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.038313051408846034
    },
    "custom|mmlu:security_studies|0": {
      "acc": 0.3183673469387755,
      "acc_stderr": 0.02982253379398205,
      "acc_norm": 0.27346938775510204,
      "acc_norm_stderr": 0.02853556033712844
    },
    "custom|mmlu:sociology|0": {
      "acc": 0.2835820895522388,
      "acc_stderr": 0.031871875379197966,
      "acc_norm": 0.29850746268656714,
      "acc_norm_stderr": 0.032357437893550424
    },
    "custom|mmlu:us_foreign_policy|0": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "custom|mmlu:virology|0": {
      "acc": 0.25903614457831325,
      "acc_stderr": 0.034106466140718564,
      "acc_norm": 0.3253012048192771,
      "acc_norm_stderr": 0.03647168523683227
    },
    "custom|mmlu:world_religions|0": {
      "acc": 0.4678362573099415,
      "acc_stderr": 0.038268824176603704,
      "acc_norm": 0.47953216374269003,
      "acc_norm_stderr": 0.0383161053282193
    },
    "custom|openbookqa|0": {
      "acc": 0.22,
      "acc_stderr": 0.01854421137582033,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.02148775108972052
    },
    "custom|piqa|0": {
      "acc": 0.691,
      "acc_stderr": 0.014619600977206484,
      "acc_norm": 0.698,
      "acc_norm_stderr": 0.014526080235459538
    },
    "custom|siqa|0": {
      "acc": 0.368,
      "acc_stderr": 0.0152580735615218,
      "acc_norm": 0.395,
      "acc_norm_stderr": 0.015466551464829344
    },
    "custom|winogrande|0": {
      "acc": 0.514,
      "acc_stderr": 0.01581309754773099,
      "acc_norm": 0.505,
      "acc_norm_stderr": 0.01581850894443666
    },
    "custom|arc:_average|0": {
      "acc": 0.489,
      "acc_stderr": 0.014790246947120986,
      "acc_norm": 0.482,
      "acc_norm_stderr": 0.01504858580609663
    },
    "custom|mmlu:_average|0": {
      "acc": 0.3015640982715279,
      "acc_stderr": 0.03378497626521098,
      "acc_norm": 0.3177975722342204,
      "acc_norm_stderr": 0.03439803970767086
    },
    "all": {
      "acc": 0.31838697848426295,
      "acc_stderr": 0.03153898378843723,
      "acc_norm": 0.3354840248823164,
      "acc_norm_stderr": 0.032131619343272876
    }
  },
  "versions": {
    "custom|arc:challenge|0": 0,
    "custom|arc:easy|0": 0,
    "custom|commonsense_qa|0": 0,
    "custom|hellaswag|0": 0,
    "custom|mmlu:abstract_algebra|0": 0,
    "custom|mmlu:anatomy|0": 0,
    "custom|mmlu:astronomy|0": 0,
    "custom|mmlu:business_ethics|0": 0,
    "custom|mmlu:clinical_knowledge|0": 0,
    "custom|mmlu:college_biology|0": 0,
    "custom|mmlu:college_chemistry|0": 0,
    "custom|mmlu:college_computer_science|0": 0,
    "custom|mmlu:college_mathematics|0": 0,
    "custom|mmlu:college_medicine|0": 0,
    "custom|mmlu:college_physics|0": 0,
    "custom|mmlu:computer_security|0": 0,
    "custom|mmlu:conceptual_physics|0": 0,
    "custom|mmlu:econometrics|0": 0,
    "custom|mmlu:electrical_engineering|0": 0,
    "custom|mmlu:elementary_mathematics|0": 0,
    "custom|mmlu:formal_logic|0": 0,
    "custom|mmlu:global_facts|0": 0,
    "custom|mmlu:high_school_biology|0": 0,
    "custom|mmlu:high_school_chemistry|0": 0,
    "custom|mmlu:high_school_computer_science|0": 0,
    "custom|mmlu:high_school_european_history|0": 0,
    "custom|mmlu:high_school_geography|0": 0,
    "custom|mmlu:high_school_government_and_politics|0": 0,
    "custom|mmlu:high_school_macroeconomics|0": 0,
    "custom|mmlu:high_school_mathematics|0": 0,
    "custom|mmlu:high_school_microeconomics|0": 0,
    "custom|mmlu:high_school_physics|0": 0,
    "custom|mmlu:high_school_psychology|0": 0,
    "custom|mmlu:high_school_statistics|0": 0,
    "custom|mmlu:high_school_us_history|0": 0,
    "custom|mmlu:high_school_world_history|0": 0,
    "custom|mmlu:human_aging|0": 0,
    "custom|mmlu:human_sexuality|0": 0,
    "custom|mmlu:international_law|0": 0,
    "custom|mmlu:jurisprudence|0": 0,
    "custom|mmlu:logical_fallacies|0": 0,
    "custom|mmlu:machine_learning|0": 0,
    "custom|mmlu:management|0": 0,
    "custom|mmlu:marketing|0": 0,
    "custom|mmlu:medical_genetics|0": 0,
    "custom|mmlu:miscellaneous|0": 0,
    "custom|mmlu:moral_disputes|0": 0,
    "custom|mmlu:moral_scenarios|0": 0,
    "custom|mmlu:nutrition|0": 0,
    "custom|mmlu:philosophy|0": 0,
    "custom|mmlu:prehistory|0": 0,
    "custom|mmlu:professional_accounting|0": 0,
    "custom|mmlu:professional_law|0": 0,
    "custom|mmlu:professional_medicine|0": 0,
    "custom|mmlu:professional_psychology|0": 0,
    "custom|mmlu:public_relations|0": 0,
    "custom|mmlu:security_studies|0": 0,
    "custom|mmlu:sociology|0": 0,
    "custom|mmlu:us_foreign_policy|0": 0,
    "custom|mmlu:virology|0": 0,
    "custom|mmlu:world_religions|0": 0,
    "custom|openbookqa|0": 0,
    "custom|piqa|0": 0,
    "custom|siqa|0": 0,
    "custom|winogrande|0": 0
  },
  "config_tasks": {
    "custom|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|arc:easy": {
      "name": "arc:easy",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Easy",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": 1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 2376,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|commonsense_qa": {
      "name": "commonsense_qa",
      "prompt_function": "commonsense_qa_prompt",
      "hf_repo": "commonsense_qa",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1221,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_prompt",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:abstract_algebra": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:anatomy": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:astronomy": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:business_ethics": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:clinical_knowledge": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:college_biology": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:college_chemistry": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:college_computer_science": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:college_mathematics": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:college_medicine": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:college_physics": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:computer_security": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:conceptual_physics": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:econometrics": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:electrical_engineering": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:elementary_mathematics": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:formal_logic": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:global_facts": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_biology": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_chemistry": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_computer_science": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_european_history": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_geography": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_government_and_politics": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_macroeconomics": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_mathematics": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_microeconomics": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_physics": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_psychology": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_statistics": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_us_history": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:high_school_world_history": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:human_aging": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:human_sexuality": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:international_law": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:jurisprudence": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:logical_fallacies": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:machine_learning": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:management": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:marketing": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:medical_genetics": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:miscellaneous": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:moral_disputes": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:moral_scenarios": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:nutrition": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:philosophy": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:prehistory": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:professional_accounting": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:professional_law": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:professional_medicine": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:professional_psychology": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:public_relations": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:security_studies": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:sociology": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:us_foreign_policy": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:virology": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|mmlu:world_religions": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_prompt",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|openbookqa": {
      "name": "openbookqa",
      "prompt_function": "openbookqa",
      "hf_repo": "openbookqa",
      "hf_subset": "main",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 500,
      "effective_num_docs": 500,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|piqa": {
      "name": "piqa",
      "prompt_function": "piqa_harness",
      "hf_repo": "piqa",
      "hf_subset": "plain_text",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1838,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|siqa": {
      "name": "siqa",
      "prompt_function": "siqa_prompt",
      "hf_repo": "lighteval/siqa",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1954,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "custom|winogrande": {
      "name": "winogrande",
      "prompt_function": "winogrande",
      "hf_repo": "winogrande",
      "hf_subset": "winogrande_xl",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "validation",
        "test"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "custom"
      ],
      "original_num_docs": 1267,
      "effective_num_docs": 1000,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "custom|arc:challenge|0": {
      "hashes": {
        "hash_examples": "b726818328851f0f",
        "hash_full_prompts": "b726818328851f0f",
        "hash_input_tokens": "4c039a68486a2f18",
        "hash_cont_tokens": "329af6d45f9c383c"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 3999,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|arc:easy|0": {
      "hashes": {
        "hash_examples": "1eb0394ca8caeaa6",
        "hash_full_prompts": "1eb0394ca8caeaa6",
        "hash_input_tokens": "27493bbcba5750b9",
        "hash_cont_tokens": "fec2a0e4394913b4"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 3999,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|commonsense_qa|0": {
      "hashes": {
        "hash_examples": "bf495c38b2745fe1",
        "hash_full_prompts": "bf495c38b2745fe1",
        "hash_input_tokens": "21ff9c275112c9ea",
        "hash_cont_tokens": "d85319d7457e427f"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 5000,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|hellaswag|0": {
      "hashes": {
        "hash_examples": "2790438d79fd8c4d",
        "hash_full_prompts": "2790438d79fd8c4d",
        "hash_input_tokens": "8a0752dabb82944a",
        "hash_cont_tokens": "1ae6e68c9fe50c4d"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 4000,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "ff00c12a680621ba",
        "hash_full_prompts": "ff00c12a680621ba",
        "hash_input_tokens": "335b27c34705ab2f",
        "hash_cont_tokens": "1823127576ad46ac"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:anatomy|0": {
      "hashes": {
        "hash_examples": "7f9c9593991d6727",
        "hash_full_prompts": "7f9c9593991d6727",
        "hash_input_tokens": "6a470ceadbe0bdac",
        "hash_cont_tokens": "5cb4867d9e353377"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:astronomy|0": {
      "hashes": {
        "hash_examples": "ff5985a306787836",
        "hash_full_prompts": "ff5985a306787836",
        "hash_input_tokens": "6f5558bf0da2fd63",
        "hash_cont_tokens": "a558cd3a08511a01"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:business_ethics|0": {
      "hashes": {
        "hash_examples": "e3fe02a23d08c2d0",
        "hash_full_prompts": "e3fe02a23d08c2d0",
        "hash_input_tokens": "91d77616dcbf44d2",
        "hash_cont_tokens": "77dd8ca64dde921b"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "9b962be8e1615cd5",
        "hash_full_prompts": "9b962be8e1615cd5",
        "hash_input_tokens": "4cc1ef779d5da4e7",
        "hash_cont_tokens": "412714dfccdd8671"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:college_biology|0": {
      "hashes": {
        "hash_examples": "17b4ca841de3a2a3",
        "hash_full_prompts": "17b4ca841de3a2a3",
        "hash_input_tokens": "9248e7342e36d61b",
        "hash_cont_tokens": "150a5bf52c0645e0"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:college_chemistry|0": {
      "hashes": {
        "hash_examples": "babea49005fd8249",
        "hash_full_prompts": "babea49005fd8249",
        "hash_input_tokens": "682970890e95db18",
        "hash_cont_tokens": "6849efa0253c9f24"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:college_computer_science|0": {
      "hashes": {
        "hash_examples": "47f4fef1846c8914",
        "hash_full_prompts": "47f4fef1846c8914",
        "hash_input_tokens": "a5a701934b719ac4",
        "hash_cont_tokens": "31689ee78d62e205"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:college_mathematics|0": {
      "hashes": {
        "hash_examples": "4d3686d599963414",
        "hash_full_prompts": "4d3686d599963414",
        "hash_input_tokens": "8036853afb3f5537",
        "hash_cont_tokens": "763a45a4a05b08f2"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:college_medicine|0": {
      "hashes": {
        "hash_examples": "7209619ecac6f235",
        "hash_full_prompts": "7209619ecac6f235",
        "hash_input_tokens": "43f98976f089c794",
        "hash_cont_tokens": "f38ce9f92cc00570"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 691,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:college_physics|0": {
      "hashes": {
        "hash_examples": "6131b6c60dd7f055",
        "hash_full_prompts": "6131b6c60dd7f055",
        "hash_input_tokens": "bce9368f4d63cb9b",
        "hash_cont_tokens": "85eeb0170c8926f1"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:computer_security|0": {
      "hashes": {
        "hash_examples": "99539c9a5bc98a59",
        "hash_full_prompts": "99539c9a5bc98a59",
        "hash_input_tokens": "30daaea49032a063",
        "hash_cont_tokens": "3556ca2afc9cd320"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "4e15015839d00858",
        "hash_full_prompts": "4e15015839d00858",
        "hash_input_tokens": "1a64cb8fc9541e32",
        "hash_cont_tokens": "79496a665e02f3b0"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:econometrics|0": {
      "hashes": {
        "hash_examples": "fba5c647465e89e0",
        "hash_full_prompts": "fba5c647465e89e0",
        "hash_input_tokens": "bee7f3e52817c803",
        "hash_cont_tokens": "512df53cbda11fa2"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "2db2ba0cb98cda51",
        "hash_full_prompts": "2db2ba0cb98cda51",
        "hash_input_tokens": "2e24071bafdb8777",
        "hash_cont_tokens": "457822b33125e6fa"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "f231cd5ae05742bb",
        "hash_full_prompts": "f231cd5ae05742bb",
        "hash_input_tokens": "19cd88a2fc7224c7",
        "hash_cont_tokens": "b27e53122e8c258c"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:formal_logic|0": {
      "hashes": {
        "hash_examples": "b69d9607d5da536e",
        "hash_full_prompts": "b69d9607d5da536e",
        "hash_input_tokens": "c30aeeca31797d8a",
        "hash_cont_tokens": "38198d213e78e5d0"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:global_facts|0": {
      "hashes": {
        "hash_examples": "c9f53772e672f6bc",
        "hash_full_prompts": "c9f53772e672f6bc",
        "hash_input_tokens": "14d14adb1d513a5d",
        "hash_cont_tokens": "92d89fbe6e16042e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_biology|0": {
      "hashes": {
        "hash_examples": "455027cf6cdd02bc",
        "hash_full_prompts": "455027cf6cdd02bc",
        "hash_input_tokens": "22853941da44fc4c",
        "hash_cont_tokens": "b2955747eec682cc"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "95d9caac9edbc34d",
        "hash_full_prompts": "95d9caac9edbc34d",
        "hash_input_tokens": "168da674401cc7d4",
        "hash_cont_tokens": "02ab7cbc459ee7dd"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "6e44706db3791e51",
        "hash_full_prompts": "6e44706db3791e51",
        "hash_input_tokens": "86c4e080c977b5c6",
        "hash_cont_tokens": "8612e11cebc1bc32"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "9078fce41897117d",
        "hash_full_prompts": "9078fce41897117d",
        "hash_input_tokens": "d0beabc0f0c77d2f",
        "hash_cont_tokens": "ff15520dccfb6638"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_geography|0": {
      "hashes": {
        "hash_examples": "c68adcc34130a2e8",
        "hash_full_prompts": "c68adcc34130a2e8",
        "hash_input_tokens": "7e6c5ff3a6f97bf6",
        "hash_cont_tokens": "deb8950d1c648f6b"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "6f839b19e49a0858",
        "hash_full_prompts": "6f839b19e49a0858",
        "hash_input_tokens": "43f35e3203ae1c26",
        "hash_cont_tokens": "b74c9a7e4c8a21ed"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "708a5c05e7adb220",
        "hash_full_prompts": "708a5c05e7adb220",
        "hash_input_tokens": "b721154fab889e3c",
        "hash_cont_tokens": "da02fa939ee67eaf"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "47c621dd61fd7790",
        "hash_full_prompts": "47c621dd61fd7790",
        "hash_input_tokens": "d60a58d2afae4616",
        "hash_cont_tokens": "899ed323e1fe67a0"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "3c467180d90f6371",
        "hash_full_prompts": "3c467180d90f6371",
        "hash_input_tokens": "f22fe147d98be1fe",
        "hash_cont_tokens": "2c814c5199e1e2ca"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_physics|0": {
      "hashes": {
        "hash_examples": "89a598cdde43be79",
        "hash_full_prompts": "89a598cdde43be79",
        "hash_input_tokens": "b769f575d5b54ef3",
        "hash_cont_tokens": "0d8d3f41a82cdf44"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "d8a7bf3f17ec12d0",
        "hash_full_prompts": "d8a7bf3f17ec12d0",
        "hash_input_tokens": "d3dd69a6a4f96f91",
        "hash_cont_tokens": "e4910e1b4b2511ff"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2175,
      "non_padded": 5,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "995374a6caaa97d6",
        "hash_full_prompts": "995374a6caaa97d6",
        "hash_input_tokens": "88004ba2f3b35cc9",
        "hash_cont_tokens": "3c2fb3fe4e97139a"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 846,
      "non_padded": 18,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "7893e9d07e34cb37",
        "hash_full_prompts": "7893e9d07e34cb37",
        "hash_input_tokens": "2aa13d66673e3181",
        "hash_cont_tokens": "a32d5ae5e056471d"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "48879684e37d1716",
        "hash_full_prompts": "48879684e37d1716",
        "hash_input_tokens": "2915511e7fd16f45",
        "hash_cont_tokens": "452121267bc5f024"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:human_aging|0": {
      "hashes": {
        "hash_examples": "afae8c53bd6e5f44",
        "hash_full_prompts": "afae8c53bd6e5f44",
        "hash_input_tokens": "bf74965b35121f2a",
        "hash_cont_tokens": "e801033ffb9f56a9"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:human_sexuality|0": {
      "hashes": {
        "hash_examples": "9701f02004912a7a",
        "hash_full_prompts": "9701f02004912a7a",
        "hash_input_tokens": "a6c763b98eee4f5f",
        "hash_cont_tokens": "09d48cd0c8e38165"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 523,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:international_law|0": {
      "hashes": {
        "hash_examples": "47955196de2d2c7a",
        "hash_full_prompts": "47955196de2d2c7a",
        "hash_input_tokens": "9c0dbe4a76929752",
        "hash_cont_tokens": "6e11c1a60a8436cd"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 483,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:jurisprudence|0": {
      "hashes": {
        "hash_examples": "a992eac2b8ae8bc4",
        "hash_full_prompts": "a992eac2b8ae8bc4",
        "hash_input_tokens": "1d171beb0e9e1908",
        "hash_cont_tokens": "efbd70abf31d838e"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 431,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "b0d31ed08f699e6c",
        "hash_full_prompts": "b0d31ed08f699e6c",
        "hash_input_tokens": "5c33ef8043210771",
        "hash_cont_tokens": "3483fa1e96eae1d7"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:machine_learning|0": {
      "hashes": {
        "hash_examples": "dccdef2bae4461a6",
        "hash_full_prompts": "dccdef2bae4461a6",
        "hash_input_tokens": "7b54ab5834d865d3",
        "hash_cont_tokens": "a4622ef3a1351ea5"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 444,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:management|0": {
      "hashes": {
        "hash_examples": "f600be25303e1fe2",
        "hash_full_prompts": "f600be25303e1fe2",
        "hash_input_tokens": "e7a3672b5083e346",
        "hash_cont_tokens": "54a894ed62bca0e7"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:marketing|0": {
      "hashes": {
        "hash_examples": "1a0df3ae5e306669",
        "hash_full_prompts": "1a0df3ae5e306669",
        "hash_input_tokens": "f136aacbca418012",
        "hash_cont_tokens": "caf0834b7819fbd0"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 936,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:medical_genetics|0": {
      "hashes": {
        "hash_examples": "eb87c9cfd9b7c760",
        "hash_full_prompts": "eb87c9cfd9b7c760",
        "hash_input_tokens": "a391dc4ef7ec071f",
        "hash_cont_tokens": "6ac4487226e9261a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 399,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:miscellaneous|0": {
      "hashes": {
        "hash_examples": "f88d724036ba03b7",
        "hash_full_prompts": "f88d724036ba03b7",
        "hash_input_tokens": "aacbfd2b765e9fce",
        "hash_cont_tokens": "7e7c51e18a7ed2ad"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3127,
      "non_padded": 5,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:moral_disputes|0": {
      "hashes": {
        "hash_examples": "79782c0823005e7b",
        "hash_full_prompts": "79782c0823005e7b",
        "hash_input_tokens": "fd3675386c662a30",
        "hash_cont_tokens": "aaddcc1ff62b83bd"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1376,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "fe57fdd86442b483",
        "hash_full_prompts": "fe57fdd86442b483",
        "hash_input_tokens": "3543a98d83df4342",
        "hash_cont_tokens": "47cfd58b65c76124"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:nutrition|0": {
      "hashes": {
        "hash_examples": "421f206f5957e90f",
        "hash_full_prompts": "421f206f5957e90f",
        "hash_input_tokens": "46f33d2da0575935",
        "hash_cont_tokens": "4691eb45e51eaed2"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1221,
      "non_padded": 3,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:philosophy|0": {
      "hashes": {
        "hash_examples": "c93073e383957fc4",
        "hash_full_prompts": "c93073e383957fc4",
        "hash_input_tokens": "608752e33c3011d1",
        "hash_cont_tokens": "7b0d0c036006cacb"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1242,
      "non_padded": 2,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:prehistory|0": {
      "hashes": {
        "hash_examples": "58ec03e20eae9f90",
        "hash_full_prompts": "58ec03e20eae9f90",
        "hash_input_tokens": "1ece982d40a2534a",
        "hash_cont_tokens": "5357aff5e4007e93"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1293,
      "non_padded": 3,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:professional_accounting|0": {
      "hashes": {
        "hash_examples": "13f7a6023a118512",
        "hash_full_prompts": "13f7a6023a118512",
        "hash_input_tokens": "964e7272887a76b9",
        "hash_cont_tokens": "a8c68107021b15b2"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1118,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:professional_law|0": {
      "hashes": {
        "hash_examples": "db8cd4ba98f00e3d",
        "hash_full_prompts": "db8cd4ba98f00e3d",
        "hash_input_tokens": "d2b00f3de701001b",
        "hash_cont_tokens": "ba0e34c11f3fb6fa"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 3996,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:professional_medicine|0": {
      "hashes": {
        "hash_examples": "f0b30b4e786eaeea",
        "hash_full_prompts": "f0b30b4e786eaeea",
        "hash_input_tokens": "f6e7d15bcbc0f80b",
        "hash_cont_tokens": "466aaa0224598acb"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1087,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:professional_psychology|0": {
      "hashes": {
        "hash_examples": "29fd2b4a194c28ea",
        "hash_full_prompts": "29fd2b4a194c28ea",
        "hash_input_tokens": "6dfce9dc9c5c5b46",
        "hash_cont_tokens": "f84cbb0b127589d6"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2431,
      "non_padded": 17,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:public_relations|0": {
      "hashes": {
        "hash_examples": "52a84bb75dd812eb",
        "hash_full_prompts": "52a84bb75dd812eb",
        "hash_input_tokens": "820874936876ab92",
        "hash_cont_tokens": "0d101d06d17f23dc"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 440,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:security_studies|0": {
      "hashes": {
        "hash_examples": "2469ceb06f350432",
        "hash_full_prompts": "2469ceb06f350432",
        "hash_input_tokens": "25d8fd1cc44f4c89",
        "hash_cont_tokens": "52e58fd1f2d0a780"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 972,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:sociology|0": {
      "hashes": {
        "hash_examples": "d2060dedb3fc2bea",
        "hash_full_prompts": "d2060dedb3fc2bea",
        "hash_input_tokens": "97b197aed7ccaa77",
        "hash_cont_tokens": "9196d4f878688e48"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 804,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "84e882e740d43f01",
        "hash_full_prompts": "84e882e740d43f01",
        "hash_input_tokens": "4f12cca513c5fc74",
        "hash_cont_tokens": "49308cfb0ab6a5d7"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:virology|0": {
      "hashes": {
        "hash_examples": "0428d2d277aa56aa",
        "hash_full_prompts": "0428d2d277aa56aa",
        "hash_input_tokens": "e38d3f8b76e9ad99",
        "hash_cont_tokens": "dca629ab49e1b7a5"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 654,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|mmlu:world_religions|0": {
      "hashes": {
        "hash_examples": "7e8f045c67ba6ba1",
        "hash_full_prompts": "7e8f045c67ba6ba1",
        "hash_input_tokens": "9e8e200ab4fa71f9",
        "hash_cont_tokens": "dd293b8062807d13"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 673,
      "non_padded": 11,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|openbookqa|0": {
      "hashes": {
        "hash_examples": "fd427af2ef0577e3",
        "hash_full_prompts": "fd427af2ef0577e3",
        "hash_input_tokens": "8d403fcf2460b790",
        "hash_cont_tokens": "ed810a390cc0601a"
      },
      "truncated": 0,
      "non_truncated": 500,
      "padded": 1992,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|piqa|0": {
      "hashes": {
        "hash_examples": "0fbca46b129eb514",
        "hash_full_prompts": "0fbca46b129eb514",
        "hash_input_tokens": "d1b090d41be1b558",
        "hash_cont_tokens": "0e9222e6380262d5"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 1890,
      "non_padded": 110,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|siqa|0": {
      "hashes": {
        "hash_examples": "f4616054ab58e063",
        "hash_full_prompts": "f4616054ab58e063",
        "hash_input_tokens": "b3f67c0659c11052",
        "hash_cont_tokens": "396c1c02250d4e95"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 2758,
      "non_padded": 242,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "custom|winogrande|0": {
      "hashes": {
        "hash_examples": "fed99ea13d2a8553",
        "hash_full_prompts": "fed99ea13d2a8553",
        "hash_input_tokens": "b417b4b1b0a4aac7",
        "hash_cont_tokens": "76dd4f40644ed188"
      },
      "truncated": 0,
      "non_truncated": 1000,
      "padded": 1948,
      "non_padded": 52,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "129021397c088071",
      "hash_full_prompts": "129021397c088071",
      "hash_input_tokens": "3c6e9bc5861b2d2c",
      "hash_cont_tokens": "17f8be6442794334"
    },
    "truncated": 0,
    "non_truncated": 21008,
    "padded": 79504,
    "non_padded": 526,
    "num_truncated_few_shots": 0
  }
}